\documentclass[10pt,twocolumn]{article}
\usepackage{abstract}
\usepackage[hypcap]{caption}
\usepackage{color}
\usepackage{enumerate}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{microtype}
\usepackage{amsmath}

\nonstopmode

\hypersetup{hidelinks}

\newcommand{\term}[1]{\textit{#1}}
\newcommand{\bterm}[1]{\textbf{\textit{#1}}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\super}[1]{\textsuperscript{#1}}
\newcommand{\sub}[1]{\textsubscript{#1}}
\newcommand{\abs}[1]{\ensuremath{\left\vert#1\right\vert}}
\def\ppt{\textit{PPT}}
\def\obf{\ensuremath{\mathcal{O}}}
\def\negl{\text{negl}}

% title
\title{Algorithm obfuscation}
\date{}
\author{
  \begin{tabular}{c c c}
    Christopher Martin \\
    \small \tt{chris.martin@gatech.edu}
  \end{tabular}
}

% document
\begin{document}
\thispagestyle{empty}

\twocolumn[
\maketitle
\begin{onecolabstract}
  \begin{quote}\begin{quote}\begin{quote}
  u can't obfuscate programs
  \end{quote}\end{quote}\end{quote}
  \vspace{1em}
\end{onecolabstract}
]

\section{Introduction}

\begin{quote}
``Anyone who has tried to determine what operation is accomplished by someone else's machine language
program knows that $E$ itself (i.e., what $E$ does) can be hard to infer from an algorithm for $E$.'
 --- Diffie and Hellman, 1976 \cite{newDirections}
\end{quote}

%Intuitively, software practitioners know that obfuscating an algorithm

\section{Applications}

From \cite{onThe(Im)possibility}:

Protection from reverse engineering.

Watermarking software (cites ``cf., [CT, NSS]'') -
software vendor adds some behavior to uniquely identify the
customer, and makes it hard to remove.

Convert any public key system into a homomorphic system
(write an algorithm that uses the private key, then obfuscate it).

Random oracles (I don't understand this one).

Convert any symmetric encryption into asymmetric encryption.
\cite{newDirections} suggested using obfuscation for public key cryptography.

\section{Virtual black box}

The ``virtual black box'' property gives a strong definition of the hiding behavior we would desire
in an obfuscator $\obf$.
An adversary with an obfuscated program $\obf(M)$ can, of course, execute it, which provides the same
power as oracle access.
We would like this to be the full extent of the adversary's power; it should not be able to discern
any additional information about $M$ by inspecting the obfuscated code.

It is not immediately obvious how to formalize the notion of \textit{any additional information} ---
What properties of the original program need to be hidden?
\cite{onThe(Im)possibility} proves that regardless of how this question is answered,
a general obfuscator is unattainable.
The most lenient answer, resulting in the weakest definition of obfuscation, restricts the discussion
to predicates $P$. If $\obf$ obfuscates, then an adversary given $\obf(M)$ should have no advantage
at determining $P(M)$ over any algorithm given oracle access to $M$ (in other words, an algorithm
that runs $\obf(M)$ as a black box without inspecting it).

Formally: $\obf$ has the black box property iff for any $\ppt$ adversary $A$,
$\exists$ a $\ppt$ simulator $S$ such that
$\forall$ Turing machines $M$,
$S$ simulates the output of $A(\obf(M))$ using only oracle access to $M$:
\[ \abs{ \Pr[ A(\obf(M)) = 1 ] - \Pr[ S^M(1^\abs{M}) = 1 ] } = \negl(\abs{M}) \]

This means that every predicate $P(M)$ either can be determined by using $\obf(M)$ as a black box
or cannot be determined from $\obf(M)$ at all.
\section{Impossibility}

seriously

\section{Point functions}

useless

\bibliographystyle{IEEEtran}
\bibliography{paper}

\end{document}
